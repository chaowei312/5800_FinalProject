{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e57990bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Download NLTK data if needed\n",
    "try:\n",
    "    nltk.data.find('stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    \n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "54ec862a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 1/3 from sst2_train.pkl → saved to ../../data/processed/sst2_train_one_third.pkl, size = 18192, category = movies\n",
      "Sampled 1/3 from sst2_test.pkl → saved to ../../data/processed/sst2_test_one_third.pkl, size = 2274, category = movies\n",
      "Sampled 1/3 from sst2_val.pkl → saved to ../../data/processed/sst2_val_one_third.pkl, size = 2274, category = movies\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def load_pkl(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_pkl(obj, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def sample_one_third(data_dict, seed=42):\n",
    "    \"\"\"Randomly sample 1/3 of the dataset.\"\"\"\n",
    "    n = len(data_dict[\"texts\"])\n",
    "    k = n // 3\n",
    "    np.random.seed(seed)\n",
    "    idx = np.random.choice(n, k, replace=False)\n",
    "    \n",
    "    # subset each key\n",
    "    new_data = {k: [v[i] for i in idx] for k, v in data_dict.items()}\n",
    "    return new_data\n",
    "\n",
    "# paths\n",
    "root = \"../../data/processed/\"\n",
    "files = [\"sst2_train.pkl\", \"sst2_test.pkl\", \"sst2_val.pkl\"]\n",
    "\n",
    "for f in files:\n",
    "    data = load_pkl(root + f)\n",
    "    sampled_data = sample_one_third(data)\n",
    "\n",
    "    n = len(sampled_data[\"texts\"])\n",
    "    sampled_data[\"category\"] = [\"movies\"] * n  # or \"movie_review\"\n",
    "\n",
    "    # save sampled version\n",
    "    out_path = root + f.replace(\".pkl\", \"_one_third.pkl\")\n",
    "    save_pkl(sampled_data, out_path)\n",
    "    \n",
    "    print(f\"Sampled 1/3 from {f} → saved to {out_path}, size = {n}, category = movies\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5653333a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Amazon rows: 3599994\n",
      "DONE:\n",
      "amazon_train.pkl: 18192\n",
      "amazon_internal_val.pkl: 2274\n",
      "amazon_val.pkl: 2274\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def save_pkl(obj, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "# target sizes from SST-2 sample\n",
    "SIZES = {\n",
    "    \"train\": 18192,\n",
    "    \"internal_val\": 2274,\n",
    "    \"val\": 2274\n",
    "}\n",
    "\n",
    "# load amazon_polarity\n",
    "dataset = load_dataset(\"mteb/amazon_polarity\")[\"train\"]\n",
    "\n",
    "print(\"Total Amazon rows:\", len(dataset))\n",
    "\n",
    "# shuffle indices for reproducibility\n",
    "rng = np.random.default_rng(seed=42)\n",
    "indices = rng.permutation(len(dataset))\n",
    "\n",
    "# sampling function\n",
    "def build_amazon_split(n, start_idx):\n",
    "    idx = indices[start_idx:start_idx + n]\n",
    "    subset = dataset.select(idx.tolist())\n",
    "\n",
    "    texts = subset[\"text\"]\n",
    "    labels = subset[\"label\"]\n",
    "    label_texts = subset[\"label_text\"]\n",
    "\n",
    "    data = {\n",
    "        \"texts\": texts,\n",
    "        \"processed_texts\": texts,  # raw text retained for now\n",
    "        \"labels\": labels,          # KEEP sentiment label\n",
    "        \"label_texts\": label_texts, # KEEP sentiment text label\n",
    "        \"text_lengths\": [len(t.split()) for t in texts],\n",
    "        \"word_counts\":  [len(t.split()) for t in texts],\n",
    "        \"category\": [\"online_shopping\"] * len(texts)\n",
    "    }\n",
    "\n",
    "    return data\n",
    "\n",
    "root = \"../../data/processed/\"\n",
    "\n",
    "# sequential index allocation\n",
    "start = 0\n",
    "\n",
    "amazon_train = build_amazon_split(SIZES[\"train\"], start)\n",
    "save_pkl(amazon_train, root + \"amazon_train.pkl\")\n",
    "start += SIZES[\"train\"]\n",
    "\n",
    "amazon_internal = build_amazon_split(SIZES[\"internal_val\"], start)\n",
    "save_pkl(amazon_internal, root + \"amazon_internal_val.pkl\")\n",
    "start += SIZES[\"internal_val\"]\n",
    "\n",
    "amazon_val = build_amazon_split(SIZES[\"val\"], start)\n",
    "save_pkl(amazon_val, root + \"amazon_val.pkl\")\n",
    "\n",
    "print(\"DONE:\")\n",
    "print(\"amazon_train.pkl:\", len(amazon_train[\"texts\"]))\n",
    "print(\"amazon_internal_val.pkl:\", len(amazon_internal[\"texts\"]))\n",
    "print(\"amazon_val.pkl:\", len(amazon_val[\"texts\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "59e598cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text, lowercase=True, normalize_whitespace=True):\n",
    "    \"\"\"\n",
    "    Preprocess text for sentiment classification.\n",
    "    \"\"\"\n",
    "    if normalize_whitespace:\n",
    "        text = ' '.join(text.split())\n",
    "    \n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "    \n",
    "    # Normalize repeated punctuation\n",
    "    text = re.sub(r'([!?.]){2,}', r'\\1', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "68e52e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing amazon_train.pkl ...\n",
      "\n",
      "Processing amazon_internal_val.pkl ...\n",
      "\n",
      "Processing amazon_val.pkl ...\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def load_pkl(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_pkl(obj, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "\n",
    "# Paths\n",
    "root = \"../../data/processed/\"\n",
    "files = {\n",
    "    \"amazon_train.pkl\": \"amazon_train_preprocessed.pkl\",\n",
    "    \"amazon_internal_val.pkl\": \"amazon_internal_val_preprocessed.pkl\",\n",
    "    \"amazon_val.pkl\": \"amazon_val_preprocessed.pkl\"\n",
    "}\n",
    "\n",
    "for src, dst in files.items():\n",
    "    print(f\"\\nProcessing {src} ...\")\n",
    "    \n",
    "    data = load_pkl(root + src)\n",
    "    \n",
    "    texts = data[\"texts\"]  # original texts\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    processed = [preprocess_text(t) for t in texts]\n",
    "    \n",
    "    # Add new field\n",
    "    data[\"processed_texts\"] = processed\n",
    "    \n",
    "    # Save updated version\n",
    "    save_pkl(data, root + dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64c95683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing train …\n",
      "Saved train: 18192 samples → ../../data/processed/yelp_train_one_third.pkl\n",
      "\n",
      "Processing internal_val …\n",
      "Saved internal_val: 2274 samples → ../../data/processed/yelp_internal_val_one_third.pkl\n",
      "\n",
      "Processing val …\n",
      "Saved val: 2274 samples → ../../data/processed/yelp_val_one_third.pkl\n"
     ]
    }
   ],
   "source": [
    "def load_pkl(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_pkl(obj, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "# Desired sizes (same as SST-2 one-third)\n",
    "TARGET_SIZES = {\n",
    "    \"train\": 18192,\n",
    "    \"internal_val\": 2274,\n",
    "    \"val\": 2274\n",
    "}\n",
    "\n",
    "root = \"../../data/processed/\"\n",
    "\n",
    "# Input Yelp files\n",
    "files = {\n",
    "    \"train\": root + \"yelp_train.pkl\",\n",
    "    \"internal_val\": root + \"yelp_test.pkl\",\n",
    "    \"val\": root + \"yelp_val.pkl\"\n",
    "}\n",
    "\n",
    "# Output Yelp sampled files\n",
    "out_files = {\n",
    "    \"train\": root + \"yelp_train_one_third.pkl\",\n",
    "    \"internal_val\": root + \"yelp_internal_val_one_third.pkl\",\n",
    "    \"val\": root + \"yelp_val_one_third.pkl\"\n",
    "}\n",
    "\n",
    "# Sampling function\n",
    "def sample_dataset(data, n, seed=42):\n",
    "    total = len(data[\"texts\"])\n",
    "    if n > total:\n",
    "        raise ValueError(f\"Requested {n} samples, but dataset only has {total}!\")\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = rng.choice(total, n, replace=False)\n",
    "\n",
    "    # subset all keys\n",
    "    sampled = {k: [v[i] for i in idx] for k, v in data.items()}\n",
    "    sampled[\"category\"] = [\"local_business_review\"] * n\n",
    "    return sampled\n",
    "\n",
    "# Perform sampling\n",
    "for split in [\"train\", \"internal_val\", \"val\"]:\n",
    "    print(f\"\\nProcessing {split} …\")\n",
    "\n",
    "    data = load_pkl(files[split])\n",
    "    n = TARGET_SIZES[split]\n",
    "\n",
    "    sampled_data = sample_dataset(data, n)\n",
    "    save_pkl(sampled_data, out_files[split])\n",
    "\n",
    "    print(f\"Saved {split}: {n} samples → {out_files[split]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e467582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added category=local_business_review → yelp_train_one_third.pkl (size=18192)\n",
      "Added category=local_business_review → yelp_internal_val_one_third.pkl (size=2274)\n",
      "Added category=local_business_review → yelp_val_one_third.pkl (size=2274)\n"
     ]
    }
   ],
   "source": [
    "def load_pkl(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_pkl(obj, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "root = \"../../data/processed/\"\n",
    "\n",
    "files = [\n",
    "    \"yelp_train_one_third.pkl\",\n",
    "    \"yelp_internal_val_one_third.pkl\",\n",
    "    \"yelp_val_one_third.pkl\",\n",
    "]\n",
    "\n",
    "for fname in files:\n",
    "    path = root + fname\n",
    "    \n",
    "    data = load_pkl(path)\n",
    "    \n",
    "    n = len(data[\"texts\"])\n",
    "    data[\"category\"] = [\"local_business_review\"] * n\n",
    "    \n",
    "    save_pkl(data, path)\n",
    "    \n",
    "    print(f\"Added category=local_business_review → {fname} (size={n})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f0e65db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Building train split ===\n",
      "Saved → ..\\..\\data\\multi\\multi_train.pkl (size=54576)\n",
      "\n",
      "=== Building internal_val split ===\n",
      "Saved → ..\\..\\data\\multi\\multi_internal_val.pkl (size=6822)\n",
      "\n",
      "=== Building val split ===\n",
      "Saved → ..\\..\\data\\multi\\multi_val.pkl (size=6822)\n",
      "\n",
      "All multi-domain datasets completed!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path(\"../../data/processed/\")\n",
    "out_root = Path(\"../../data/multi/\")\n",
    "out_root.mkdir(exist_ok=True)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Helper: load pickle\n",
    "# ---------------------------------------------------\n",
    "def load_pkl(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Helper: save pickle\n",
    "# ---------------------------------------------------\n",
    "def save_pkl(obj, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "    print(f\"Saved → {path} (size={len(obj['texts'])})\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Helper: normalize structure\n",
    "# ---------------------------------------------------\n",
    "def to_list(x, length=None):\n",
    "    \"\"\"Convert HuggingFace column / numpy array / scalar to Python list.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    try:\n",
    "        return list(x)\n",
    "    except TypeError:\n",
    "        return [x] * length\n",
    "\n",
    "\n",
    "def normalize(d, domain_name):\n",
    "    \"\"\"Ensure all required fields exist and are lists.\"\"\"\n",
    "\n",
    "    d = dict(d)\n",
    "    n = len(d[\"texts\"])\n",
    "\n",
    "    # Convert all fields into lists\n",
    "    for key, val in d.items():\n",
    "        d[key] = to_list(val, length=n)\n",
    "\n",
    "    # Force category\n",
    "    d[\"category\"] = [domain_name] * n\n",
    "\n",
    "    # Optional fields\n",
    "    d[\"token_lengths\"] = to_list(d.get(\"token_lengths\", [None] * n), length=n)\n",
    "    d[\"label_texts\"] = to_list(d.get(\"label_texts\", [None] * n), length=n)\n",
    "\n",
    "    return d\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Merge utility (FORCE MERGE SPECIFIC FIELDS)\n",
    "# ---------------------------------------------------\n",
    "def merge_three(d1, d2, d3):\n",
    "\n",
    "    keys_to_merge = [\n",
    "        \"texts\",\n",
    "        \"processed_texts\",\n",
    "        \"labels\",\n",
    "        \"word_counts\",\n",
    "        \"category\"\n",
    "    ]\n",
    "\n",
    "    out = {}\n",
    "    for key in keys_to_merge:\n",
    "        out[key] = d1[key] + d2[key] + d3[key]\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Load dataset paths\n",
    "# ---------------------------------------------------\n",
    "files = {\n",
    "    \"train\": {\n",
    "        \"movie\": root/\"sst2_train_one_third.pkl\",\n",
    "        \"amazon\": root/\"amazon_train.pkl\",\n",
    "        \"yelp\": root/\"yelp_train_one_third.pkl\",\n",
    "    },\n",
    "    \"internal_val\": {\n",
    "        \"movie\": root/\"sst2_internal_val_one_third.pkl\",\n",
    "        \"amazon\": root/\"amazon_internal_val.pkl\",\n",
    "        \"yelp\": root/\"yelp_internal_val_one_third.pkl\",\n",
    "    },\n",
    "    \"val\": {\n",
    "        \"movie\": root/\"sst2_val_one_third.pkl\",\n",
    "        \"amazon\": root/\"amazon_val.pkl\",\n",
    "        \"yelp\": root/\"yelp_val_one_third.pkl\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Build multi-domain datasets\n",
    "# ---------------------------------------------------\n",
    "def build_split(split_name):\n",
    "    print(f\"\\n=== Building {split_name} split ===\")\n",
    "\n",
    "    movie = normalize(load_pkl(files[split_name][\"movie\"]), \"movie_review\")\n",
    "    amazon = normalize(load_pkl(files[split_name][\"amazon\"]), \"online_shopping\")\n",
    "    yelp = normalize(load_pkl(files[split_name][\"yelp\"]), \"local_business_review\")\n",
    "\n",
    "    merged = merge_three(movie, amazon, yelp)\n",
    "\n",
    "    save_pkl(merged, out_root / f\"multi_{split_name}.pkl\")\n",
    "\n",
    "\n",
    "# Build all splits\n",
    "build_split(\"train\")\n",
    "build_split(\"internal_val\")\n",
    "build_split(\"val\")\n",
    "\n",
    "print(\"\\nAll multi-domain datasets completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6e401102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Train Label Distribution (n=54576) ===\n",
      "Label 0: 26258 (48.11%)\n",
      "Label 1: 28318 (51.89%)\n",
      "\n",
      "=== Val Label Distribution (n=6822) ===\n",
      "Label 0: 3244 (47.55%)\n",
      "Label 1: 3578 (52.45%)\n",
      "\n",
      "=== Test Label Distribution (n=6822) ===\n",
      "Label 0: 3264 (47.85%)\n",
      "Label 1: 3558 (52.15%)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "def load_pkl(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "split_train = out_root/\"multi_train.pkl\"\n",
    "split_val   = out_root/\"multi_val.pkl\"\n",
    "split_test  = out_root/\"multi_test.pkl\"\n",
    "\n",
    "def show_label_dist(name, path):\n",
    "    data = load_pkl(path)   \n",
    "    labels = data[\"labels\"]\n",
    "    counter = Counter(labels)\n",
    "    total = len(labels)\n",
    "    \n",
    "    print(f\"\\n=== {name} Label Distribution (n={total}) ===\")\n",
    "    for lbl, cnt in sorted(counter.items()):\n",
    "        print(f\"Label {lbl}: {cnt} ({cnt/total:.2%})\")\n",
    "\n",
    "# call functions\n",
    "show_label_dist(\"Train\", split_train)\n",
    "show_label_dist(\"Val\", split_val)\n",
    "show_label_dist(\"Test\", split_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsan6600",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
