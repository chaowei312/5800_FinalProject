{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e42abbec",
   "metadata": {},
   "source": [
    "# **Comparing Baseline and Recurrent Transformers on different subset sizes of the SST-2 dataset**\n",
    "\n",
    "This notebook compares the Baseline and Recurrent transformer architectures on different subsets of the SST-2 dataset:\n",
    "   \n",
    "   1. **10% of the training data**\n",
    "   2. **50% of the training data** \n",
    "\n",
    "The goal is to analyze how data quantity affects the performance, convergence, and efficiency of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed754d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "import copy\n",
    "from tqdm.notebook import tqdm # Use tqdm.notebook for Jupyter progress bars\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c2ad01",
   "metadata": {},
   "source": [
    "## 1. Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd947579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configurations \n",
    "BASELINE_CONFIG = {\n",
    "    'hidden_size': 384,\n",
    "    'num_hidden_layers': 6,\n",
    "    'num_attention_heads': 6,\n",
    "    'intermediate_size': 1536,\n",
    "    'dropout_prob': 0.1,\n",
    "    'use_flash_attention': True,\n",
    "    'use_swiglu': True,\n",
    "    'use_rope': True,\n",
    "    'use_rms_norm': True\n",
    "}\n",
    "\n",
    "RECURRENT_CONFIG = {\n",
    "    'hidden_size': 256,\n",
    "    'num_hidden_layers': 3,\n",
    "    'recurrent_depth': 2,  # Effective depth: 3 Ã— 2 = 6\n",
    "    'num_attention_heads': 4,\n",
    "    'intermediate_size': 1024,\n",
    "    'dropout_prob': 0.1,\n",
    "    'residual_scale': 0.5,\n",
    "    'use_flash_attention': True,\n",
    "    'use_swiglu': True,\n",
    "    'use_rope': True,\n",
    "    'use_rms_norm': True\n",
    "}\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    'num_epochs': 5,\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 3e-5,\n",
    "    'warmup_steps': 100,\n",
    "    'eval_steps': 50,\n",
    "    'max_length': 128,\n",
    "    'patience': 3, # For early stopping\n",
    "    'min_delta': 0.001 # For early stopping\n",
    "}\n",
    "\n",
    "DATA_SUBSETS = ['10_percent', '50_percent']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03082e68",
   "metadata": {},
   "source": [
    "## 2. Functions for Models, Training, and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da67b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# get project root\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "# add to python path\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "\n",
    "from training.utils import prepare_sst2_data, load_tokenizer\n",
    "from models.baseline import BaselineModel, BaselineConfig\n",
    "from models.recurrent import RecurrentModel, RecurrentConfig\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def get_models():\n",
    "    \"\"\"Initializes and returns fresh instances of the models.\"\"\"\n",
    "    # Baseline model\n",
    "    baseline_config = BaselineConfig(\n",
    "        vocab_size=30522,\n",
    "        hidden_size=BASELINE_CONFIG['hidden_size'],\n",
    "        num_hidden_layers=BASELINE_CONFIG['num_hidden_layers'],\n",
    "        num_attention_heads=BASELINE_CONFIG['num_attention_heads'],\n",
    "        intermediate_size=BASELINE_CONFIG['intermediate_size'],\n",
    "        hidden_dropout_prob=BASELINE_CONFIG['dropout_prob'],\n",
    "        attention_probs_dropout_prob=BASELINE_CONFIG['dropout_prob'],\n",
    "        num_labels=2,\n",
    "        **{k: v for k, v in BASELINE_CONFIG.items() if k.startswith('use_')}\n",
    "    )\n",
    "\n",
    "    # Recurrent model\n",
    "    recurrent_config = RecurrentConfig(\n",
    "        vocab_size=30522,\n",
    "        hidden_size=RECURRENT_CONFIG['hidden_size'],\n",
    "        num_hidden_layers=RECURRENT_CONFIG['num_hidden_layers'],\n",
    "        recurrent_depth=RECURRENT_CONFIG['recurrent_depth'],\n",
    "        num_attention_heads=RECURRENT_CONFIG['num_attention_heads'],\n",
    "        intermediate_size=RECURRENT_CONFIG['intermediate_size'],\n",
    "        hidden_dropout_prob=RECURRENT_CONFIG['dropout_prob'],\n",
    "        attention_probs_dropout_prob=RECURRENT_CONFIG['dropout_prob'],\n",
    "        residual_scale=RECURRENT_CONFIG['residual_scale'],\n",
    "        num_labels=2,\n",
    "        **{k: v for k, v in RECURRENT_CONFIG.items() if k.startswith('use_')}\n",
    "    )\n",
    "    \n",
    "    baseline_model = BaselineModel(baseline_config).to(device)\n",
    "    recurrent_model = RecurrentModel(recurrent_config).to(device)\n",
    "    \n",
    "    return baseline_model, recurrent_model\n",
    "\n",
    "def train_model(model, name, train_loader, val_loader, num_epochs, patience, min_delta):\n",
    "    optimizer = AdamW(model.parameters(), lr=TRAINING_CONFIG['learning_rate'])\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=False)\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        progress_bar = tqdm(train_loader, desc=f\"{name} Epoch {epoch+1}\", leave=False)\n",
    "        for batch in progress_bar:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['labels'])\n",
    "            loss = outputs['loss']\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            progress_bar.set_postfix(loss=np.mean(train_losses))\n",
    "        \n",
    "        model.eval()\n",
    "        val_losses, val_correct, val_total = [], 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['labels'])\n",
    "                val_losses.append(outputs['loss'].item())\n",
    "                predictions = outputs['logits'].argmax(dim=-1)\n",
    "                val_correct += (predictions == batch['labels']).sum().item()\n",
    "                val_total += batch['labels'].size(0)\n",
    "        \n",
    "        train_loss, val_loss, val_acc = np.mean(train_losses), np.mean(val_losses), val_correct / val_total\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "            \n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"Loaded best model with val_loss={best_val_loss:.4f}\")\n",
    "        \n",
    "    return history\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    all_predictions, all_labels, inference_times = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            start_time = time.time()\n",
    "            outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "            inference_times.append((time.time() - start_time) * 1000 / batch['input_ids'].size(0))\n",
    "            predictions = outputs['logits'].argmax(dim=-1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "            \n",
    "    accuracy = (np.array(all_predictions) == np.array(all_labels)).mean()\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1_score(all_labels, all_predictions, average='weighted'),\n",
    "        'precision': precision_score(all_labels, all_predictions, average='weighted'),\n",
    "        'recall': recall_score(all_labels, all_predictions, average='weighted'),\n",
    "        'inference_time_ms': np.mean(inference_times)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac829a3",
   "metadata": {},
   "source": [
    "## 3. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84608444",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer('bert-base-uncased')\n",
    "all_results = []\n",
    "training_histories = {}\n",
    "\n",
    "for subset in DATA_SUBSETS:\n",
    "    print(f\"\\n{'='*20} Experiment: {subset.replace('_', ' ').title()} {'='*20}\")\n",
    "    data_dir = f'../data/processed_size_splits_sst2/{subset}'\n",
    "    training_histories[subset] = {}\n",
    "    \n",
    "    # Load data for the current subset\n",
    "    train_loader, val_loader, test_loader = prepare_sst2_data(\n",
    "        data_dir=data_dir,\n",
    "        tokenizer=tokenizer,\n",
    "        batch_size=TRAINING_CONFIG['batch_size'],\n",
    "        max_length=TRAINING_CONFIG['max_length']\n",
    "    )\n",
    "    print(f\"Dataset sizes - Train: {len(train_loader.dataset)}, Validation: {len(val_loader.dataset)}, Test: {len(test_loader.dataset)}\")\n",
    "\n",
    "    # Get fresh models\n",
    "    baseline_model, recurrent_model = get_models()\n",
    "    models_to_run = {\n",
    "        'Baseline': baseline_model,\n",
    "        'Recurrent': recurrent_model\n",
    "    }\n",
    "\n",
    "    for model_name, model in models_to_run.items():\n",
    "        print(f\"\\n--- Training {model_name} on {subset} ---\")\n",
    "        \n",
    "        # Train the model\n",
    "        history = train_model(\n",
    "            model, f\"{model_name}-{subset}\", train_loader, val_loader, \n",
    "            num_epochs=TRAINING_CONFIG['num_epochs'],\n",
    "            patience=TRAINING_CONFIG['patience'],\n",
    "            min_delta=TRAINING_CONFIG['min_delta']\n",
    "        )\n",
    "        training_histories[subset][model_name] = history\n",
    "        \n",
    "        # Evaluate the model\n",
    "        print(f\"--- Evaluating {model_name} on {subset} test set ---\")\n",
    "        metrics = evaluate_model(model, test_loader)\n",
    "        \n",
    "        # Store results\n",
    "        params = count_parameters(model)\n",
    "        size_mb = params * 4 / (1024 * 1024)\n",
    "        result = {\n",
    "            'Model': model_name,\n",
    "            'Subset': subset,\n",
    "            'Parameters': params,\n",
    "            'Size (MB)': size_mb,\n",
    "            **metrics\n",
    "        }\n",
    "        all_results.append(result)\n",
    "        print(f\"Results for {model_name} on {subset}: Accuracy={metrics['accuracy']:.4f}, F1={metrics['f1']:.4f}\")\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d6a066",
   "metadata": {},
   "source": [
    "## 4. Analyze and Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800a48c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Final Comparison Table ---\")\n",
    "display(results_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd5d8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(len(DATA_SUBSETS), 2, figsize=(14, 6 * len(DATA_SUBSETS)), sharex='col')\n",
    "fig.suptitle('Training Dynamics Across Data Subsets', fontsize=16, y=1.02)\n",
    "\n",
    "for i, subset in enumerate(DATA_SUBSETS):\n",
    "    ax_loss = axes[i][0]\n",
    "    ax_acc = axes[i][1]\n",
    "    \n",
    "    # Plot loss\n",
    "    baseline_hist = training_histories[subset]['Baseline']\n",
    "    recurrent_hist = training_histories[subset]['Recurrent']\n",
    "    \n",
    "    ax_loss.plot(baseline_hist['train_loss'], label='Baseline Train', color='blue', linestyle='-')\n",
    "    ax_loss.plot(baseline_hist['val_loss'], label='Baseline Val', color='blue', linestyle='--')\n",
    "    ax_loss.plot(recurrent_hist['train_loss'], label='Recurrent Train', color='orange', linestyle='-')\n",
    "    ax_loss.plot(recurrent_hist['val_loss'], label='Recurrent Val', color='orange', linestyle='--')\n",
    "    ax_loss.set_ylabel('Loss')\n",
    "    ax_loss.set_title(f'Loss on {subset.replace(\"_\", \" \").title()}')\n",
    "    ax_loss.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    ax_acc.plot(baseline_hist['val_acc'], label='Baseline', color='blue', marker='o')\n",
    "    ax_acc.plot(recurrent_hist['val_acc'], label='Recurrent', color='orange', marker='s')\n",
    "    ax_acc.set_ylabel('Validation Accuracy')\n",
    "    ax_acc.set_title(f'Accuracy on {subset.replace(\"_\", \" \").title()}')\n",
    "    ax_acc.legend()\n",
    "\n",
    "axes[-1][0].set_xlabel('Epoch')\n",
    "axes[-1][1].set_xlabel('Epoch')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e84d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bubble plot for performance comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "markers = {\n",
    "    '10_percent': 'o', # Circle\n",
    "    '50_percent': 's'  # Square\n",
    "}\n",
    "colors = {\n",
    "    'Baseline': 'blue',\n",
    "    'Recurrent': 'orange'\n",
    "}\n",
    "\n",
    "for i, row in results_df.iterrows():\n",
    "    plt.scatter(\n",
    "        row['Size (MB)'],\n",
    "        row['Accuracy'],\n",
    "        s=row['inference_time_ms'] * 150, # Bubble size by inference time\n",
    "        c=colors[row['Model']],\n",
    "        marker=markers[row['Subset']],\n",
    "        alpha=0.6,\n",
    "        label=f\"{row['Model']} {row['Subset']}\" if i < 4 else \"\" # Avoid duplicate labels\n",
    "    )\n",
    "    plt.text(row['Size (MB)']+0.5, row['Accuracy'], f\"{row['Model']}\\n{row['Subset'].split('_')[0]}\", fontsize=9)\n",
    "\n",
    "# Create custom legend\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', color='w', label='10% Data', markerfacecolor='gray', markersize=10),\n",
    "    Line2D([0], [0], marker='s', color='w', label='50% Data', markerfacecolor='gray', markersize=10),\n",
    "    Line2D([0], [0], color='blue', lw=4, label='Baseline Model'),\n",
    "    Line2D([0], [0], color='orange', lw=4, label='Recurrent Model')\n",
    "]\n",
    "\n",
    "plt.xlabel('Model Size (MB)')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Performance vs. Model Size Across Data Subsets\\n(Bubble Size ~ Inference Time)')\n",
    "plt.legend(handles=legend_elements, loc='lower right')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsan5400",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
