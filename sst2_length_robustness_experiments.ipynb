{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "731313cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd947579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configurations\n",
    "BASELINE_CONFIG = {\n",
    "    'hidden_size': 384,\n",
    "    'num_hidden_layers': 6,\n",
    "    'num_attention_heads': 6,\n",
    "    'intermediate_size': 1536,\n",
    "    'dropout_prob': 0.1,\n",
    "    'use_flash_attention': True,\n",
    "    'use_swiglu': True,\n",
    "    'use_rope': True,\n",
    "    'use_rms_norm': True\n",
    "}\n",
    "\n",
    "RECURRENT_CONFIG = {\n",
    "    'hidden_size': 256,\n",
    "    'num_hidden_layers': 3,\n",
    "    'recurrent_depth': 2,  # Effective depth: 3 × 2 = 6\n",
    "    'num_attention_heads': 4,\n",
    "    'intermediate_size': 1024,\n",
    "    'dropout_prob': 0.1,\n",
    "    'residual_scale': 0.5,\n",
    "    'use_flash_attention': True,\n",
    "    'use_swiglu': True,\n",
    "    'use_rope': True,\n",
    "    'use_rms_norm': True\n",
    "}\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    'num_epochs': 5,\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 3e-5,\n",
    "    'warmup_steps': 100,\n",
    "    'eval_steps': 50,\n",
    "    'max_length': 128\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0da67b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6398 samples from data/processed_length_splits_sst2/long\\sst2_train.pkl\n",
      "Loaded 336 samples from data/processed_length_splits_sst2/long\\sst2_internal_val.pkl\n",
      "Loaded 87 samples from data/processed_length_splits_sst2/long\\sst2_val.pkl\n",
      "Dataset sizes - Train: 6400, Validation: 336\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "from training.utils import prepare_sst2_data, load_tokenizer\n",
    "\n",
    "tokenizer = load_tokenizer('bert-base-uncased')\n",
    "train_loader, val_loader, test_loader = prepare_sst2_data(\n",
    "    data_dir='data/processed_length_splits_sst2/long',\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=TRAINING_CONFIG['batch_size'],\n",
    "    max_length=TRAINING_CONFIG['max_length']\n",
    ")\n",
    "\n",
    "train_size = len(train_loader) * TRAINING_CONFIG['batch_size']\n",
    "val_size = len(val_loader) * TRAINING_CONFIG['batch_size']\n",
    "print(f\"Dataset sizes - Train: {train_size}, Validation: {val_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84608444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 25,912,706 parameters (25.9M)\n",
      "Recurrent: 10,972,162 parameters (11.0M)\n",
      "Parameter reduction: 57.7%\n"
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "from models.baseline import BaselineModel, BaselineConfig\n",
    "from models.recurrent import RecurrentModel, RecurrentConfig\n",
    "\n",
    "# Baseline model\n",
    "baseline_config = BaselineConfig(\n",
    "    vocab_size=30522,\n",
    "    hidden_size=BASELINE_CONFIG['hidden_size'],\n",
    "    num_hidden_layers=BASELINE_CONFIG['num_hidden_layers'],\n",
    "    num_attention_heads=BASELINE_CONFIG['num_attention_heads'],\n",
    "    intermediate_size=BASELINE_CONFIG['intermediate_size'],\n",
    "    hidden_dropout_prob=BASELINE_CONFIG['dropout_prob'],\n",
    "    attention_probs_dropout_prob=BASELINE_CONFIG['dropout_prob'],\n",
    "    num_labels=2,\n",
    "    **{k: v for k, v in BASELINE_CONFIG.items() if k.startswith('use_')}\n",
    ")\n",
    "\n",
    "# Recurrent model\n",
    "recurrent_config = RecurrentConfig(\n",
    "    vocab_size=30522,\n",
    "    hidden_size=RECURRENT_CONFIG['hidden_size'],\n",
    "    num_hidden_layers=RECURRENT_CONFIG['num_hidden_layers'],\n",
    "    recurrent_depth=RECURRENT_CONFIG['recurrent_depth'],\n",
    "    num_attention_heads=RECURRENT_CONFIG['num_attention_heads'],\n",
    "    intermediate_size=RECURRENT_CONFIG['intermediate_size'],\n",
    "    hidden_dropout_prob=RECURRENT_CONFIG['dropout_prob'],\n",
    "    attention_probs_dropout_prob=RECURRENT_CONFIG['dropout_prob'],\n",
    "    residual_scale=RECURRENT_CONFIG['residual_scale'],\n",
    "    num_labels=2,\n",
    "    **{k: v for k, v in RECURRENT_CONFIG.items() if k.startswith('use_')}\n",
    ")\n",
    "\n",
    "baseline_model = BaselineModel(baseline_config).to(device)\n",
    "recurrent_model = RecurrentModel(recurrent_config).to(device)\n",
    "\n",
    "# Model statistics\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "baseline_params = count_parameters(baseline_model)\n",
    "recurrent_params = count_parameters(recurrent_model)\n",
    "\n",
    "print(f\"Baseline: {baseline_params:,} parameters ({baseline_params/1e6:.1f}M)\")\n",
    "print(f\"Recurrent: {recurrent_params:,} parameters ({recurrent_params/1e6:.1f}M)\")\n",
    "print(f\"Parameter reduction: {(1 - recurrent_params/baseline_params)*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800bbda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with early stopping (patience=3)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Epoch 1: Train=0.2184, Val=0.4766, Acc=0.8810 *\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Baseline Epoch 2/5:  55%|█████▌    | 220/400 [13:20<04:21,  1.45s/it]"
     ]
    }
   ],
   "source": [
    "# Train both models with early stopping\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "def train_model(model, name, num_epochs=TRAINING_CONFIG['num_epochs'], patience=3, min_delta=0.001):\n",
    "    optimizer = AdamW(model.parameters(), lr=TRAINING_CONFIG['learning_rate'])\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=False)\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'lr': []}\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"{name} Epoch {epoch+1}/{num_epochs}\", leave=False):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                labels=batch['labels']\n",
    "            )\n",
    "            \n",
    "            loss = outputs['loss']\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['labels']\n",
    "                )\n",
    "                \n",
    "                val_losses.append(outputs['loss'].item())\n",
    "                predictions = outputs['logits'].argmax(dim=-1)\n",
    "                val_correct += (predictions == batch['labels']).sum().item()\n",
    "                val_total += batch['labels'].size(0)\n",
    "        \n",
    "        train_loss = np.mean(train_losses)\n",
    "        val_loss = np.mean(val_losses)\n",
    "        val_acc = val_correct / val_total\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "            \n",
    "            print(f\"{name} Epoch {epoch+1}: Train={train_loss:.4f}, Val={val_loss:.4f}, Acc={val_acc:.4f} *\")\n",
    "\n",
    "            # Save best model at this moment\n",
    "            save_path = f\"{name}_best_model.pt\"\n",
    "            torch.save(best_model_state, save_path)\n",
    "            # print(f\"{name}: Saved new best model → {save_path}\")   # Optional log\n",
    "\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"{name} Epoch {epoch+1}: Train={train_loss:.4f}, Val={val_loss:.4f}, Acc={val_acc:.4f}\")\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"{name}: Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"{name}: Loaded best model with val_loss={best_val_loss:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Train models\n",
    "print(\"Training with early stopping (patience=3)\")\n",
    "print(\"-\" * 50)\n",
    "baseline_history = train_model(baseline_model, \"Baseline\")\n",
    "print()\n",
    "recurrent_history = train_model(recurrent_model, \"Recurrent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062a2ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    inference_times = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Measure inference time\n",
    "            start_time = time.time()\n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask']\n",
    "            )\n",
    "            inference_times.append((time.time() - start_time) * 1000 / batch['input_ids'].size(0))\n",
    "            \n",
    "            predictions = outputs['logits'].argmax(dim=-1)\n",
    "            total_correct += (predictions == batch['labels']).sum().item()\n",
    "            total_samples += batch['labels'].size(0)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "    \n",
    "    accuracy = total_correct / total_samples\n",
    "    avg_inference_time = np.mean(inference_times)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'inference_time_ms': avg_inference_time\n",
    "    }\n",
    "\n",
    "# Evaluate on test set\n",
    "baseline_metrics = evaluate_model(baseline_model, test_loader)\n",
    "recurrent_metrics = evaluate_model(recurrent_model, test_loader)\n",
    "\n",
    "# Model sizes\n",
    "baseline_size_mb = baseline_params * 4 / (1024 * 1024)  # 4 bytes per parameter\n",
    "recurrent_size_mb = recurrent_params * 4 / (1024 * 1024)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Baseline', 'Recurrent'],\n",
    "    'Parameters': [baseline_params, recurrent_params],\n",
    "    'Size (MB)': [baseline_size_mb, recurrent_size_mb],\n",
    "    'Accuracy': [baseline_metrics['accuracy'], recurrent_metrics['accuracy']],\n",
    "    'F1': [baseline_metrics['f1'], recurrent_metrics['f1']],\n",
    "    'Precision': [baseline_metrics['precision'], recurrent_metrics['precision']],\n",
    "    'Recall': [baseline_metrics['recall'], recurrent_metrics['recall']],\n",
    "    'Inference (ms)': [baseline_metrics['inference_time_ms'], recurrent_metrics['inference_time_ms']]\n",
    "})\n",
    "\n",
    "print(comparison_df.round(4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsan6600",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
